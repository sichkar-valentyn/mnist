
<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/mnist/assets/css/style.css?v=f5115139db52d20db14e8e20cfd470512e23b4b7">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>MNIST Digits Classification with numpy only | mnist</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="MNIST Digits Classification with numpy only" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MNIST Digits Classification with numpy only" />
<meta property="og:description" content="MNIST Digits Classification with numpy only" />
<link rel="canonical" href="https://sichkar-valentyn.github.io/mnist/" />
<meta property="og:url" content="https://sichkar-valentyn.github.io/mnist/" />
<meta property="og:site_name" content="mnist" />
<script type="application/ld+json">
{"@type":"WebSite","headline":"MNIST Digits Classification with numpy only","url":"https://sichkar-valentyn.github.io/mnist/","name":"mnist","description":"MNIST Digits Classification with numpy only","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          
            <a id="forkme_banner" href="https://github.com/sichkar-valentyn/mnist">View on GitHub</a>
          

          <h1 id="project_title">mnist</h1>
          <h2 id="project_tagline">MNIST Digits Classification with numpy only</h2>

          
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1 id="mnist-digits-classification-with-numpy-only">MNIST Digits Classification with <code class="highlighter-rouge">numpy</code> only</h1>
<p>Example on Digits Classification with the help of MNIST dataset of handwritten digits and Convolutional Neural Network.
<br /><a href="https://doi.org/10.5281/zenodo.1317904"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.1317904.svg" alt="DOI" /></a></p>

<h2 id="test-online-here">Test online <a href="https://valentynsichkar.name/mnist.html">here</a></h2>

<h2 id="content">Content</h2>
<p>Theory and experimental results (on this page)</p>

<ul>
  <li><a href="#mnist-digits-classification-with-numpy-library-only">MNIST Digits Classification with <code class="highlighter-rouge">numpy</code> only</a>
    <ul>
      <li><a href="#loading-mnist-dataset">Loading MNIST dataset</a></li>
      <li><a href="#plotting-examples-of-digits-from-mnist-dataset">Plotting examples of digits from MNIST dataset</a></li>
      <li><a href="#preprocessing-loaded-mnist-dataset">Preprocessing loaded MNIST dataset</a></li>
      <li><a href="#saving-and-loading-serialized-models">Saving and Loading serialized models</a></li>
      <li><a href="#functions-for-dealing-with-cnn-layers">Functions for dealing with CNN layers</a>
        <ul>
          <li><a href="#naive-forward-pass-for-convolutional-layer">Naive Forward Pass for Convolutional layer</a></li>
          <li><a href="#naive-backward-pass-for-convolutional-layer">Naive Backward Pass for Convolutional layer</a></li>
          <li><a href="#naive-forward-pass-for-max-pooling-layer">Naive Forward Pass for Max Pooling layer</a></li>
          <li><a href="#naive-backward-pass-for-max-pooling-layer">Naive Backward Pass for Max Pooling layer</a></li>
          <li><a href="#forward-pass-for-affine-layer">Forward Pass for Affine layer</a></li>
          <li><a href="#backward-pass-for-affine-layer">Backward Pass for Affine layer</a></li>
          <li><a href="#forward-pass-for-relu-layer">Forward Pass for ReLU layer</a></li>
          <li><a href="#backward-pass-for-relu-layer">Backward Pass for ReLU layer</a></li>
          <li><a href="#softmax-classification-loss">Softmax Classification loss</a></li>
        </ul>
      </li>
      <li><a href="#creating-classifier-model-of-cnn">Creating Classifier - model of CNN</a>
        <ul>
          <li><a href="#initializing-new-network">Initializing new Network</a></li>
          <li><a href="#evaluating-loss-for-training-convnet1">Evaluating loss for training ConvNet1</a></li>
          <li><a href="#calculating-scores-for-predicting-convnet1">Calculating scores for predicting ConvNet1</a></li>
        </ul>
      </li>
      <li><a href="#optimization-functions">Functions for Optimization</a>
        <ul>
          <li><a href="#vanilla-sgd">Vanilla SGD</a></li>
          <li><a href="#momentum-sgd">Momentum SGD</a></li>
          <li><a href="#rms-propagation">RMS Propagation</a></li>
          <li><a href="#adam">Adam</a></li>
        </ul>
      </li>
      <li><a href="#creating-solver-class">Creating Solver Class</a>
        <ul>
          <li><a href="#reset">_Reset</a></li>
          <li><a href="#step">_Step</a></li>
          <li><a href="#accuracy">Checking Accuracy</a></li>
          <li><a href="#train">Train</a></li>
        </ul>
      </li>
      <li><a href="#overfitting-small-data">Overfitting Small Data</a></li>
      <li><a href="#training-results">Training Results</a></li>
      <li><a href="#full-codes">Full Codes</a></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="mnist-digits-classification-with-numpy-only-1"><a id="mnist-digits-classification-with-numpy-library-only">MNIST Digits Classification with <code class="highlighter-rouge">numpy</code> only</a></h3>
<p>In this example we’ll test CNN for Digits Classification with the help of MNIST dataset.
<br />Following standard and most common parameters can be used and tested:</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Weights Initialization</td>
      <td>HE Normal</td>
    </tr>
    <tr>
      <td>Weights Update Policy</td>
      <td>Vanilla SGD, Momentum SGD, RMSProp, Adam</td>
    </tr>
    <tr>
      <td>Activation Functions</td>
      <td>ReLU, Sigmoid</td>
    </tr>
    <tr>
      <td>Regularization</td>
      <td>L2, Dropout</td>
    </tr>
    <tr>
      <td>Pooling</td>
      <td>Max, Average</td>
    </tr>
    <tr>
      <td>Loss Functions</td>
      <td>Softmax, SVM</td>
    </tr>
  </tbody>
</table>

<p><br />Contractions:</p>
<ul>
  <li><strong>Vanilla SGD</strong> - Vanilla Stochastic Gradient Descent</li>
  <li><strong>Momentum SGD</strong> - Stochastic Gradient Descent with Momentum</li>
  <li><strong>RMSProp</strong> - Root Mean Square Propagation</li>
  <li><strong>Adam</strong> - Adaptive Moment Estimation</li>
  <li><strong>SVM</strong> - Support Vector Machine</li>
</ul>

<p><br /><strong>For current example</strong> following architecture will be used:
<br /><code class="highlighter-rouge">Input</code> –&gt; <code class="highlighter-rouge">Conv</code> –&gt; <code class="highlighter-rouge">ReLU</code> –&gt; <code class="highlighter-rouge">Pool</code> –&gt; <code class="highlighter-rouge">Affine</code> –&gt; <code class="highlighter-rouge">ReLU</code> –&gt; <code class="highlighter-rouge">Affine</code> –&gt; <code class="highlighter-rouge">Softmax</code></p>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/Model_1_Architecture_MNIST.png" alt="Model_1_Architecture.png" /></p>

<p><br /><strong>For current example</strong> following parameters will be used:</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Weights Initialization</td>
      <td><code class="highlighter-rouge">HE Normal</code></td>
    </tr>
    <tr>
      <td>Weights Update Policy</td>
      <td><code class="highlighter-rouge">Vanilla SGD</code></td>
    </tr>
    <tr>
      <td>Activation Functions</td>
      <td><code class="highlighter-rouge">ReLU</code></td>
    </tr>
    <tr>
      <td>Regularization</td>
      <td><code class="highlighter-rouge">L2</code></td>
    </tr>
    <tr>
      <td>Pooling</td>
      <td><code class="highlighter-rouge">Max</code></td>
    </tr>
    <tr>
      <td>Loss Functions</td>
      <td><code class="highlighter-rouge">Softmax</code></td>
    </tr>
  </tbody>
</table>

<p><br /><strong>File structure</strong> with folders and functions can be seen on the figure below:</p>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/Image_Classification_Files_Structure.png" alt="Image_Classification_File_Structure.png" /></p>

<p><br />Also, <strong>file structure</strong> can be seen below:</p>
<ul>
  <li>MNIST Digits Classification with <code class="highlighter-rouge">numpy</code> only:
    <ul>
      <li><code class="highlighter-rouge">Data_Preprocessing</code>
        <ul>
          <li><code class="highlighter-rouge">datasets</code></li>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a></li>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/mean_and_std.pickle">mean_and_std.pickle</a></li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">Helper_Functions</code>
        <ul>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a></li>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/optimize_rules.py">optimize_rules.py</a></li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">Classifiers</code>
        <ul>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Classifiers/ConvNet1.py">ConvNet1.py</a></li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">Serialized_Models</code>
        <ul>
          <li>model1.pickle</li>
        </ul>
      </li>
      <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Solver.py">Solver.py</a></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="loading-mnist-dataset"><a id="loading-mnist-dataset">Loading MNIST dataset</a></h3>
<p>After downloading files from official resource, there has to be following files:</p>
<ul>
  <li>train-images-idx3-ubyte.gz</li>
  <li>train-labels-idx1-ubyte.gz</li>
  <li>t10k-images-idx3-ubyte.gz</li>
  <li>t10k-labels-idx1-ubyte.gz</li>
</ul>

<p>Writing code in Python.
<br />Importing needed libraries.
<br />Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""Importing library for object serialization
which we'll use for saving and loading serialized models"""</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="c"># Importing other standard libraries</span>
<span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>Creating function for loading MNIST images.
<br />Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="nb">file</span><span class="p">,</span> <span class="n">number_of_images</span><span class="p">):</span>
    <span class="c"># Opening file for reading in binary mode</span>
    <span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span> <span class="k">as</span> <span class="n">bytestream</span><span class="p">:</span>
        <span class="n">bytestream</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
        <span class="s">"""Initially testing file with images has shape (60000 * 784)
        Where, 60000 - number of image samples
        784 - one channel of image (28 x 28)
        Every image consists of 28x28 pixels with its only one channel"""</span>
        <span class="c"># Reading data</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="n">bytestream</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">number_of_images</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
        <span class="c"># Placing data in numpy array and converting it into 'float32' type</span>
        <span class="c"># It is used further in function 'pre_process_mnist' as it is needed to subtract float from float</span>
        <span class="c"># And for standard deviation as it is needed to divide float by float</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c"># Reshaping data making for every image separate matrix (28, 28)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">number_of_images</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>  <span class="c"># (60000, 28, 28)</span>

        <span class="c"># Preparing array with shape for 1 channeled image</span>
        <span class="c"># Making for every image separate matrix (28, 28, 1)</span>
        <span class="n">array_of_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">number_of_images</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c"># (60000, 28, 28, 1)</span>

        <span class="c"># Assigning to array one channeled image from dataset</span>
        <span class="c"># In this way we get normal 3-channeled images</span>
        <span class="n">array_of_image</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span>

    <span class="c"># Returning array of loaded images from file</span>
    <span class="k">return</span> <span class="n">array_of_image</span>
</code></pre></div></div>

<p>Creating function for loading MNIST labels.
<br />Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_labels</span><span class="p">(</span><span class="nb">file</span><span class="p">,</span> <span class="n">number_of_labels</span><span class="p">):</span>
    <span class="c"># Opening file for reading in binary mode</span>
    <span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span> <span class="k">as</span> <span class="n">bytestream</span><span class="p">:</span>
        <span class="n">bytestream</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
        <span class="s">"""Initially testing file with labels has shape (60000)
        Where, 60000 - number of labels"""</span>
        <span class="c"># Reading data</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="n">bytestream</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">number_of_labels</span><span class="p">)</span>
        <span class="c"># Placing data in numpy array and converting it into 'int64' type</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>  <span class="c"># (60000, )</span>

    <span class="c"># Returning array of loaded labels from file</span>
    <span class="k">return</span> <span class="n">labels</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="plotting-examples-of-digits-from-mnist-dataset"><a id="plotting-examples-of-digits-from-mnist-dataset">Plotting examples of digits from MNIST dataset</a></h3>
<p>After dataset was load, it is possible to show examples of training images.
<br />Creating function for showing first 100 unique example of images from MNIST dataset.
<br />Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_mnist_examples</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
    <span class="c"># Preparing labels for each class</span>
    <span class="c"># MNIST has 10 classes from 0 to 9</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">'0'</span><span class="p">,</span> <span class="s">'1'</span><span class="p">,</span> <span class="s">'2'</span><span class="p">,</span> <span class="s">'3'</span><span class="p">,</span> <span class="s">'4'</span><span class="p">,</span> <span class="s">'5'</span><span class="p">,</span> <span class="s">'6'</span><span class="p">,</span> <span class="s">'7'</span><span class="p">,</span> <span class="s">'8'</span><span class="p">,</span> <span class="s">'9'</span><span class="p">]</span>

    <span class="c"># Taking first ten different (unique) training images from training set</span>
    <span class="c"># Going through labels and putting their indexes into list</span>
    <span class="c"># Starting from '0' index</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c"># Defining variable for counting total amount of examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c"># Defining dictionary for storing unique label numbers and their indexes</span>
    <span class="c"># As key there is unique label</span>
    <span class="c"># As value there is a list with indexes of this label</span>
    <span class="n">d_plot</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="c"># Checking if label is already in dictionary</span>
        <span class="k">if</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">d_plot</span><span class="p">:</span>
            <span class="n">d_plot</span><span class="p">[</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">m</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c"># Else if label is already in dictionary adding index to the list</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">d_plot</span><span class="p">[</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">:</span>
            <span class="n">d_plot</span><span class="p">[</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+=</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">m</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c"># Checking if there is already ten labels for all labels</span>
        <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="mi">100</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="c"># Increasing 'i'</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c"># Preparing figures for plotting</span>
    <span class="n">figure_1</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="c"># 'ax 'is as (10, 10) np array and we can call each time ax[0, 0]</span>

    <span class="c"># Plotting first ten labels of training examples</span>
    <span class="c"># Here we plot only matrix of image with only one channel '[:, :, 0]'</span>
    <span class="c"># Showing image in grayscale specter by 'cmap=plt.get_cmap('gray')'</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">d_plot</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]][:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'gray'</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="c"># Plotting 90 rest of training examples</span>
    <span class="c"># Here we plot only matrix of image with only one channel '[:, :, 0]'</span>
    <span class="c"># Showing image in grayscale specter by 'cmap=plt.get_cmap('gray')'</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">d_plot</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]][:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'gray'</span><span class="p">))</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>

    <span class="c"># Giving the name to the window with figure</span>
    <span class="n">figure_1</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">set_window_title</span><span class="p">(</span><span class="s">'MNIST examples'</span><span class="p">)</span>
    <span class="c"># Showing the plots</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>For plotting images consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Plotting 100 examples of training images from 10 classes</span>
<span class="c"># We can't use here data after preprocessing</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s">'datasets/train-images-idx3-ubyte.gz'</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c"># (1000, 28, 28, 1)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">load_labels</span><span class="p">(</span><span class="s">'datasets/train-labels-idx1-ubyte.gz'</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c"># (1000,)</span>
<span class="c"># Also, making arrays as type of 'int' in order to show correctly on the plot</span>
<span class="n">plot_mnist_examples</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'int'</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'int'</span><span class="p">))</span>
</code></pre></div></div>

<p>Result can be seen on the image below.</p>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/MNIST_examples.png" alt="MNIST_examples" /></p>

<p><br /></p>

<h3 id="preprocessing-loaded-mnist-dataset"><a id="preprocessing-loaded-mnist-dataset">Preprocessing loaded MNIST dataset</a></h3>
<p>Next, creating function for preprocessing MNIST dataset for further use in classifier.</p>
<ul>
  <li>Normalizing data by <code class="highlighter-rouge">dividing / 255.0</code> (!) - up to researcher</li>
  <li>Normalizing data by <code class="highlighter-rouge">subtracting mean image</code> and <code class="highlighter-rouge">dividing by standard deviation</code> (!) - up to researcher</li>
  <li>Transposing every dataset to make channels come first</li>
  <li>Returning result as dictionary</li>
</ul>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pre_process_mnist</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="c"># Normalizing whole data by dividing /255.0</span>
    <span class="n">x_train</span> <span class="o">/=</span> <span class="mf">255.0</span>
    <span class="n">x_test</span> <span class="o">/=</span> <span class="mf">255.0</span>  <span class="c"># Data for testing consists of 10000 examples from testing dataset</span>

    <span class="c"># Preparing data for training, validation and testing</span>
    <span class="c"># Data for validation is taken with 1000 examples from training dataset in range from 59000 to 60000</span>
    <span class="n">batch_mask</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">59000</span><span class="p">,</span> <span class="mi">60000</span><span class="p">))</span>
    <span class="n">x_validation</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>  <span class="c"># (1000, 28, 28, 1)</span>
    <span class="n">y_validation</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>  <span class="c"># (1000,)</span>
    <span class="c"># Data for training is taken with first 59000 examples from training dataset</span>
    <span class="n">batch_mask</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">59000</span><span class="p">))</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>  <span class="c"># (59000, 28, 28, 1)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>  <span class="c"># (59000,)</span>

    <span class="c"># Normalizing data by subtracting mean image and dividing by standard deviation</span>
    <span class="c"># Subtracting the dataset by mean image serves to center the data.</span>
    <span class="c"># It helps for each feature to have a similar range and gradients don't go out of control.</span>
    <span class="c"># Calculating mean image from training dataset along the rows by specifying 'axis=0'</span>
    <span class="n">mean_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c"># numpy.ndarray (28, 28, 1)</span>

    <span class="c"># Calculating standard deviation from training dataset along the rows by specifying 'axis=0'</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c"># numpy.ndarray (28, 28, 1)</span>
    <span class="c"># Taking into account that a lot of values are 0, that is why we need to replace it to 1</span>
    <span class="c"># In order to avoid dividing by 0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">28</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">28</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">std</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">std</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="c"># Saving calculated 'mean_image' and 'std' into 'pickle' file</span>
    <span class="c"># We will use them when preprocess input data for classifying</span>
    <span class="c"># We will need to subtract and divide input image for classifying</span>
    <span class="c"># As we're doing now for training, validation and testing data</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="p">{</span><span class="s">'mean_image'</span><span class="p">:</span> <span class="n">mean_image</span><span class="p">,</span> <span class="s">'std'</span><span class="p">:</span> <span class="n">std</span><span class="p">}</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'mean_and_std.pickle'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f_mean_std</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dictionary</span><span class="p">,</span> <span class="n">f_mean_std</span><span class="p">)</span>

    <span class="c"># Subtracting calculated mean image from pre-processed datasets</span>
    <span class="n">x_train</span> <span class="o">-=</span> <span class="n">mean_image</span>
    <span class="n">x_validation</span> <span class="o">-=</span> <span class="n">mean_image</span>
    <span class="n">x_test</span> <span class="o">-=</span> <span class="n">mean_image</span>

    <span class="c"># Dividing then every dataset by standard deviation</span>
    <span class="n">x_train</span> <span class="o">/=</span> <span class="n">std</span>
    <span class="n">x_validation</span> <span class="o">/=</span> <span class="n">std</span>
    <span class="n">x_test</span> <span class="o">/=</span> <span class="n">std</span>

    <span class="c"># Transposing every dataset to make channels come first</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c"># (59000, 1, 28, 28)</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c"># (10000, 1, 28, 28)</span>
    <span class="n">x_validation</span> <span class="o">=</span> <span class="n">x_validation</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c"># (10000, 1, 28, 28)</span>

    <span class="c"># Returning result as dictionary</span>
    <span class="n">d_processed</span> <span class="o">=</span> <span class="p">{</span><span class="s">'x_train'</span><span class="p">:</span> <span class="n">x_train</span><span class="p">,</span> <span class="s">'y_train'</span><span class="p">:</span> <span class="n">y_train</span><span class="p">,</span>
                   <span class="s">'x_validation'</span><span class="p">:</span> <span class="n">x_validation</span><span class="p">,</span> <span class="s">'y_validation'</span><span class="p">:</span> <span class="n">y_validation</span><span class="p">,</span>
                   <span class="s">'x_test'</span><span class="p">:</span> <span class="n">x_test</span><span class="p">,</span> <span class="s">'y_test'</span><span class="p">:</span> <span class="n">y_test</span><span class="p">}</span>

    <span class="c"># Returning dictionary</span>
    <span class="k">return</span> <span class="n">d_processed</span>
</code></pre></div></div>

<p>After running created function, it is possible to see loaded, prepared and preprocessed CIFAR-10 datasets.
<br />Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Loading whole data for preprocessing</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s">'datasets/train-images-idx3-ubyte.gz'</span><span class="p">,</span> <span class="mi">60000</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">load_labels</span><span class="p">(</span><span class="s">'datasets/train-labels-idx1-ubyte.gz'</span><span class="p">,</span> <span class="mi">60000</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s">'datasets/t10k-images-idx3-ubyte.gz'</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">load_labels</span><span class="p">(</span><span class="s">'datasets/t10k-labels-idx1-ubyte.gz'</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="c"># Showing pre-processed data from dictionary</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pre_process_mnist</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="s">':'</span><span class="p">,</span> <span class="n">j</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>As a result there will be following:</p>
<ul>
  <li><code class="highlighter-rouge">x_train: (59000, 1, 28, 28)</code></li>
  <li><code class="highlighter-rouge">y_train: (59000,)</code></li>
  <li><code class="highlighter-rouge">x_validation: (1000, 1, 28, 28)</code></li>
  <li><code class="highlighter-rouge">y_validation: (1000,)</code></li>
  <li><code class="highlighter-rouge">x_test: (1000, 1, 28, 28)</code></li>
  <li><code class="highlighter-rouge">y_test: (1000,)</code></li>
</ul>

<p><br /></p>

<h3 id="saving-and-loading-serialized-models"><a id="saving-and-loading-serialized-models">Saving and Loading serialized models</a></h3>
<p>Saving loaded and preprocessed data into ‘pickle’ file.
<br />Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'data.pickle'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div></div>
<p><br /></p>

<h3 id="functions-for-dealing-with-cnn-layers"><a id="functions-for-dealing-with-cnn-layers">Functions for dealing with CNN layers</a></h3>
<p>Creating functions for CNN layers:</p>
<ul>
  <li>Naive Forward Pass for Convolutional layer</li>
  <li>Naive Backward Pass for Convolutional layer</li>
  <li>Naive Forward Pass for Max Pooling layer</li>
  <li>Naive Backward Pass for Max Pooling layer</li>
  <li>Forward Pass for Affine layer</li>
  <li>Backward Pass for Affine layer</li>
  <li>Forward Pass for ReLU layer</li>
  <li>Backward Pass for ReLU layer</li>
  <li>Softmax Classification loss</li>
</ul>

<h4 id="naive-forward-pass-for-convolutional-layer"><a id="naive-forward-pass-for-convolutional-layer">Naive Forward Pass for Convolutional layer</a></h4>
<p>Defining function for naive forward pass for convolutional layer.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Input consists of following:
    x of shape (N, C, H, W) - N data, each with C channels, height H and width W.
    w of shape (F, C, HH, WW) - We convolve each input with F different filters,
        where each filter spans all C channels; each filter has height HH and width WW.
    'cnn_params' is a dictionary with following keys:
        'stride' - step for sliding,
        'pad' - zero-pad frame around input.
Function returns a tuple of (out, cash):
    feature_maps - output data of feature maps of shape (N, F, H', W') where:
        H' = 1 + (H + 2 * pad - HH) / stride
        W' = 1 + (W + 2 * pad - WW) / stride
            where,
            N here is the same as we have it as number of input images,
            F here is as number of channels of each N (that are now as feature maps).
    cache - is a tuple of (x, w, b, cnn_params), needed in backward pass.
    
"""</span>
</code></pre></div></div>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cnn_forward_naive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">cnn_params</span><span class="p">):</span>
    <span class="c"># Preparing parameters for convolution operation</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">cnn_params</span><span class="p">[</span><span class="s">'stride'</span><span class="p">]</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="n">cnn_params</span><span class="p">[</span><span class="s">'pad'</span><span class="p">]</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">F</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">HH</span><span class="p">,</span> <span class="n">WW</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>

    <span class="c"># Cache for output</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">cnn_params</span><span class="p">)</span>

    <span class="c"># Applying to the input image volume Pad frame with zero values for all channels</span>
    <span class="c"># As we have in input x N as number of inputs, C as number of channels,</span>
    <span class="c"># then we don't have to pad them</span>
    <span class="c"># That's why we leave first two tuples with 0 - (0, 0), (0, 0)</span>
    <span class="c"># And two last tuples with pad parameter - (pad, pad), (pad, pad)</span>
    <span class="c"># In this way we pad only H and W of N inputs with C channels</span>
    <span class="n">x_padded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="s">'constant'</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c"># Defining spatial size of output image volume (feature maps) by following formulas:</span>
    <span class="n">height_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">HH</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">width_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">WW</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
    <span class="c"># Depth of output volume is number of filters which is F</span>
    <span class="c"># And number of input images N remains the same - it is number of output image volumes now</span>

    <span class="c"># Creating zero valued volume for output feature maps</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">height_out</span><span class="p">,</span> <span class="n">width_out</span><span class="p">))</span>

    <span class="c"># Implementing convolution through N input images, each with F filters</span>
    <span class="c"># Also, with respect to C channels</span>
    <span class="c"># For every image</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c"># For every filter</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">F</span><span class="p">):</span>
            <span class="c"># Defining variable for indexing height in output feature map</span>
            <span class="c"># (because our step might not be equal to 1)</span>
            <span class="n">height_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c"># Convolving every channel of the image with every channel of the current filter</span>
            <span class="c"># Result is summed up</span>
            <span class="c"># Going through all input image (2D convolution) through all channels</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
                <span class="c"># Defining variable for indexing width in output feature map</span>
                <span class="c"># (because our step might not be equal to 1)</span>
                <span class="n">width_index</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
                    <span class="n">feature_maps</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">height_index</span><span class="p">,</span> <span class="n">width_index</span><span class="p">]</span> <span class="o">=</span> \
                        <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x_padded</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">HH</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">WW</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:])</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">f</span><span class="p">]</span>
                    <span class="c"># Increasing index for width</span>
                    <span class="n">width_index</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="c"># Increasing index for height</span>
                <span class="n">height_index</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c"># Returning resulted volumes of feature maps and cash</span>
    <span class="k">return</span> <span class="n">feature_maps</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>

<h4 id="naive-backward-pass-for-convolutional-layer"><a id="naive-backward-pass-for-convolutional-layer">Naive Backward Pass for Convolutional layer</a></h4>
<p>Defining function for naive backward pass for convolutional layer.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Defining function for naive backward pass for convolutional layer.
Input consists of following:
    derivatives_out - Upstream derivatives.
    cache - is a tuple of (x, w, b, cnn_params) as in 'cnn_forward_naive' function:
        x of shape (N, C, H, W) - N data, each with C channels, height H and width W.
        w of shape (F, C, HH, WW) - We convolve each input with F different filters,
            where each filter spans all C channels; each filter has height HH and width WW.
        'cnn_params' is a dictionary with following keys:
            'stride' - step for sliding,
            'pad' - zero-pad frame around input.
Function returns a tuple of (dx, dw, db):
    dx - gradient with respect to x,
    dw - gradient with respect to w,
    db - gradient with respect to b.
"""</span>
</code></pre></div></div>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cnn_backward_naive</span><span class="p">(</span><span class="n">derivative_out</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="c"># Preparing variables for input, weights, biases, cnn parameters from cache</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">cnn_params</span> <span class="o">=</span> <span class="n">cache</span>

    <span class="c"># Preparing variables with appropriate shapes</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c"># For input</span>
    <span class="n">F</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">HH</span><span class="p">,</span> <span class="n">WW</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>  <span class="c"># For weights</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">height_out</span><span class="p">,</span> <span class="n">weight_out</span> <span class="o">=</span> <span class="n">derivative_out</span><span class="o">.</span><span class="n">shape</span>  <span class="c"># For output feature maps</span>

    <span class="c"># Preparing variables with parameters</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">cnn_params</span><span class="p">[</span><span class="s">'stride'</span><span class="p">]</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="n">cnn_params</span><span class="p">[</span><span class="s">'pad'</span><span class="p">]</span>

    <span class="c"># Preparing gradients for output</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="c"># It is important to remember that cash has original non-padded input x.</span>
    <span class="c"># Applying to the input image volume Pad frame with zero values for all channels</span>
    <span class="c"># As we have in input x N as number of inputs, C as number of channels,</span>
    <span class="c"># then we don't have to pad them</span>
    <span class="c"># That's why we leave first two tuples with 0 - (0, 0), (0, 0)</span>
    <span class="c"># And two last tuples with pad parameter - (pad, pad), (pad, pad)</span>
    <span class="c"># In this way we pad only H and W of N inputs with C channels</span>
    <span class="n">x_padded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="s">'constant'</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c"># The same we apply padding for dx</span>
    <span class="n">dx_padded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="s">'constant'</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c"># Implementing backward pass through N input images, each with F filters</span>
    <span class="c"># Also, with respect to C channels</span>
    <span class="c"># And calculating gradients</span>
    <span class="c"># For every image</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c"># For every filter</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">F</span><span class="p">):</span>
            <span class="c"># Going through all input image through all channels</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
                    <span class="c"># Calculating gradients</span>
                    <span class="n">dx_padded</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">HH</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">WW</span><span class="p">]</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">derivative_out</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
                    <span class="n">dw</span><span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">x_padded</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">HH</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">WW</span><span class="p">]</span> <span class="o">*</span> <span class="n">derivative_out</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
                    <span class="n">db</span><span class="p">[</span><span class="n">f</span><span class="p">]</span> <span class="o">+=</span> <span class="n">derivative_out</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

    <span class="c"># Reassigning dx by slicing dx_padded</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dx_padded</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c"># Returning calculated gradients</span>
    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></div>

<h4 id="naive-forward-pass-for-max-pooling-layer"><a id="naive-forward-pass-for-max-pooling-layer">Naive Forward Pass for Max Pooling layer</a></h4>
<p>Defining function for naive forward pass for Max Pooling layer.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Defining function for naive forward pass for Max Pooling layer.
Input consists of following:
    x as input data with shape (N, C, H, W) - N data, each with C channels, height H and width W.
    'pooling_params' is a dictionary with following keys:
        'pooling_height' - height of pooling region,
        'pooling_width' - width of pooling region,
        'stride' - step (distance) between pooling regions.
    
Function returns a tuple of (pooled_output, cache):
    pooled_output - is output resulted data of shape (N, C, H', W') where:
        H' = 1 + (H + pooling_height) / stride
        W' = 1 + (W + pooling_width) / stride
            where,
            N here is the same as we have it as number of input images,
            C here is as number of channels of each N.
    cache - is a tuple of (x, pooling_params), needed in backward pass.
"""</span>
</code></pre></div></div>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">max_pooling_forward_naive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pooling_params</span><span class="p">):</span>
    <span class="c"># Preparing variables with appropriate shapes</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c"># For input</span>

    <span class="c"># Preparing variables with parameters</span>
    <span class="n">pooling_height</span> <span class="o">=</span> <span class="n">pooling_params</span><span class="p">[</span><span class="s">'pooling_height'</span><span class="p">]</span>
    <span class="n">pooling_width</span> <span class="o">=</span> <span class="n">pooling_params</span><span class="p">[</span><span class="s">'pooling_width'</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">pooling_params</span><span class="p">[</span><span class="s">'stride'</span><span class="p">]</span>

    <span class="c"># Cache for output</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pooling_params</span><span class="p">)</span>

    <span class="c"># Defining spatial size of output image volume after pooling layer by following formulas:</span>
    <span class="n">height_pooled_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">-</span> <span class="n">pooling_height</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">width_polled_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">pooling_width</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
    <span class="c"># Depth of output volume is number of channels which is C (or number of feature maps)</span>
    <span class="c"># And number of input images N remains the same - it is number of output image volumes now</span>

    <span class="c"># Creating zero valued volume for output image volume after pooling layer</span>
    <span class="n">pooled_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">height_pooled_out</span><span class="p">,</span> <span class="n">width_polled_out</span><span class="p">))</span>

    <span class="c"># Implementing forward naive pooling pass through N input images, each with C channels</span>
    <span class="c"># And calculating output pooled image volume</span>
    <span class="c"># For every image</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c"># Going through all input image through all channels</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">height_pooled_out</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">width_polled_out</span><span class="p">):</span>
                <span class="c"># Preparing height and width for current pooling region</span>
                <span class="n">ii</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span>
                <span class="n">jj</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span>
                <span class="c"># Getting current pooling region with all channels C</span>
                <span class="n">current_pooling_region</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:,</span> <span class="n">ii</span><span class="p">:</span><span class="n">ii</span><span class="o">+</span><span class="n">pooling_height</span><span class="p">,</span> <span class="n">jj</span><span class="p">:</span><span class="n">jj</span><span class="o">+</span><span class="n">pooling_width</span><span class="p">]</span>
                <span class="c"># Finding maximum value for all channels C and filling output pooled image</span>
                <span class="c"># Reshaping current pooling region from (3, 2, 2) - 3 channels and 2 by 2</span>
                <span class="c"># To (3, 4) in order to utilize np.max function</span>
                <span class="c"># Specifying 'axis=1' as parameter for choosing maximum value out of 4 numbers along 3 channels</span>
                <span class="n">pooled_output</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> \
                    <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">current_pooling_region</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">C</span><span class="p">,</span> <span class="n">pooling_height</span> <span class="o">*</span> <span class="n">pooling_width</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c"># Returning output resulted data</span>
    <span class="k">return</span> <span class="n">pooled_output</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>

<h4 id="naive-backward-pass-for-max-pooling-layer"><a id="naive-backward-pass-for-max-pooling-layer">Naive Backward Pass for Max Pooling layer</a></h4>
<p>Defining function for naive backward pass for Max Pooling layer.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Defining function for naive backward pass for Max Pooling layer.
Input consists of following:
    derivatives_out - Upstream derivatives.
    cache - is a tuple of (x, pooling_params) as in 'max_pooling_forward_naive' function:
        x as input data with shape (N, C, H, W) - N data, each with C channels, height H and width W.
        'pooling_params' is a dictionary with following keys:
            'pooling_height' - height of pooling region,
            'pooling_width' - width of pooling region,
            'stride' - step (distance) between pooling regions.
    
Function returns:
    dx - gradient with respect to x.
"""</span>
</code></pre></div></div>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">max_pooling_backward_naive</span><span class="p">(</span><span class="n">derivatives_out</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="c"># Preparing variables with appropriate shapes</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">pooling_params</span> <span class="o">=</span> <span class="n">cache</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

    <span class="c"># Preparing variables with parameters</span>
    <span class="n">pooling_height</span> <span class="o">=</span> <span class="n">pooling_params</span><span class="p">[</span><span class="s">'pooling_height'</span><span class="p">]</span>
    <span class="n">pooling_width</span> <span class="o">=</span> <span class="n">pooling_params</span><span class="p">[</span><span class="s">'pooling_width'</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">pooling_params</span><span class="p">[</span><span class="s">'stride'</span><span class="p">]</span>

    <span class="c"># Defining spatial size of output image volume after pooling layer by following formulas:</span>
    <span class="n">height_pooled_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">-</span> <span class="n">pooling_height</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">width_polled_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">pooling_width</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
    <span class="c"># Depth of output volume is number of channels which is C (or number of feature maps)</span>
    <span class="c"># And number of input images N remains the same - it is number of output image volumes now</span>

    <span class="c"># Creating zero valued volume for output gradient after backward pass of pooling layer</span>
    <span class="c"># The shape is the same with x.shape</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">))</span>

    <span class="c"># Implementing backward naive pooling pass through N input images, each with C channels</span>
    <span class="c"># And calculating output pooled image volume</span>
    <span class="c"># For every image</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c"># For every channel</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
            <span class="c"># Going through all pooled image by height and width</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">height_pooled_out</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">width_polled_out</span><span class="p">):</span>
                    <span class="c"># Preparing height and width for current pooling region</span>
                    <span class="n">ii</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span>
                    <span class="n">jj</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span>
                    <span class="c"># Getting current pooling region</span>
                    <span class="n">current_pooling_region</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">ii</span><span class="p">:</span><span class="n">ii</span><span class="o">+</span><span class="n">pooling_height</span><span class="p">,</span> <span class="n">jj</span><span class="p">:</span><span class="n">jj</span><span class="o">+</span><span class="n">pooling_width</span><span class="p">]</span>
                    <span class="c"># Finding maximum value for current pooling region</span>
                    <span class="n">current_maximum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">current_pooling_region</span><span class="p">)</span>
                    <span class="c"># Creating array with the same shape as 'current_pooling_region'</span>
                    <span class="c"># Filling with 'True' and 'False' according to the condition '==' to 'current_maximum'</span>
                    <span class="n">temp</span> <span class="o">=</span> <span class="n">current_pooling_region</span> <span class="o">==</span> <span class="n">current_maximum</span>
                    <span class="c"># Calculating output gradient</span>
                    <span class="n">dx</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">ii</span><span class="p">:</span><span class="n">ii</span><span class="o">+</span><span class="n">pooling_height</span><span class="p">,</span> <span class="n">jj</span><span class="p">:</span><span class="n">jj</span><span class="o">+</span><span class="n">pooling_width</span><span class="p">]</span> <span class="o">+=</span> \
                        <span class="n">derivatives_out</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">temp</span>

                    <span class="c"># Backward pass for pooling layer will return gradient with respect to x</span>
                    <span class="c"># Each pooling region will be filled with '0'</span>
                    <span class="c"># or derivative if that value was maximum for forward pass</span>
                    <span class="c"># print(x[0, 0, 0:2, 0:2])</span>
                    <span class="c"># print()</span>
                    <span class="c"># print(dx[0, 0, 0:2, 0:2])</span>

                    <span class="c"># [[ 0.57775955 -0.03546282]</span>
                    <span class="c">#  [-1.03050044 -1.23398021]]</span>

                    <span class="c"># [[-0.93262122  0.        ]</span>
                    <span class="c">#  [ 0.          0.        ]]</span>

    <span class="c"># Returning gradient with respect to x</span>
    <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h4 id="forward-pass-for-affine-layer"><a id="forward-pass-for-affine-layer">Forward Pass for Affine layer</a></h4>
<p>Defining function for forward pass for Affine layer.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Defining function for computing forward pass for Affine layer.
Affine layer - this is Fully Connected layer.
Input consists of following:
    x - input data in form of numpy array and shape (N, d1, ..., dk),
    w - weights in form of numpy array and shape (D, M),
    b - biases in form of numpy array and shape (M,),
        where input x contains N batches and each batch x[i] has shape (d1, ..., dk).
        We will reshape each input batch x[i] into vector of dimension D = d1 * ... * dk.
        As a result, input will be in form of matrix with shape (N, D).
        It is needed for calculation product of input matrix over weights.
        As weights matrix has shape (D, M), then output resulted matrix will be with shape (N, M).
        
Function returns a tuple of:
    affine_output - output data in form of numpy array and shape (N, M),
    cache - is a tuple of (x, w, b), needed in backward pass.
"""</span>
</code></pre></div></div>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c"># Cache for output</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c"># Reshaping input data with N batches into matrix with N rows</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c"># By using '-1' we say that number of column is unknown, but number of rows N is known</span>
    <span class="c"># Resulted matrix will be with N rows and D columns</span>
    <span class="c"># Example:</span>
    <span class="c"># x = np.random.randint(0, 9, (2, 3, 3))</span>
    <span class="c"># print(x.shape)  # (2, 3, 3)</span>
    <span class="c"># print(x)</span>
    <span class="c">#             [[[3 6 5]</span>
    <span class="c">#               [6 3 2]</span>
    <span class="c">#               [1 0 0]]</span>
    <span class="c">#</span>
    <span class="c">#              [[8 5 8]</span>
    <span class="c">#               [7 5 2]</span>
    <span class="c">#               [2 1 6]]]</span>
    <span class="c">#</span>
    <span class="c"># x = x.reshape(2, -1)</span>
    <span class="c"># print(x.shape)  # (2, 9)</span>
    <span class="c"># print(x)</span>
    <span class="c">#             [[3 6 5 6 3 2 1 0 0]</span>
    <span class="c">#              [8 5 8 7 5 2 2 1 6]]</span>

    <span class="c"># Implementing Affine forward pass.</span>
    <span class="c"># Calculating product of input data over weights</span>
    <span class="n">affine_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

    <span class="c"># Returning resulted matrix with shape of (N, M)</span>
    <span class="k">return</span> <span class="n">affine_output</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>

<h4 id="backward-pass-for-affine-layer"><a id="backward-pass-for-affine-layer">Backward Pass for Affine layer</a></h4>
<p>Defining function for backward pass for Affine layer.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Defining function for computing backward pass for Affine layer.
Affine layer - this is Fully Connected layer.
Input consists of following:
    derivatives_out - Upstream derivatives of shape (N, M),
    cache - is a tuple of (x, w, b):
        x - input data in form of numpy array and shape (N, d1, ..., dk),
        w - weights in form of numpy array and shape (D, M),
        b - biases in form of numpy array and shape (M,).
Function returns a tuple of (dx, dw, db):
    dx - gradient with respect to x of shape (N, d1, ..., dk),
    dw - gradient with respect to w of shape (D, M),
    db - gradient with respect to b of shape (M,).
"""</span>
</code></pre></div></div>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">affine_backward</span><span class="p">(</span><span class="n">derivatives_out</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="c"># Preparing variables for input, weights and biases from cache</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cache</span>

    <span class="c"># Implementing backward pass for Affine layer</span>
    <span class="c"># Calculating gradient with respect to x and reshaping to make shape as in x</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">derivatives_out</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c"># Calculating gradient with respect to w</span>
    <span class="c"># Reshaping input data with N batches into matrix with N rows and D columns</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">derivatives_out</span><span class="p">)</span>
    <span class="c"># Calculating gradient with respect to b</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">derivatives_out</span><span class="p">)</span>
    <span class="c"># db = np.sum(derivatives_out, axis=0)</span>

    <span class="c"># Returning calculated gradients</span>
    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></div>

<h4 id="forward-pass-for-relu-layer"><a id="forward-pass-for-relu-layer">Forward Pass for ReLU layer</a></h4>
<p>Defining function for forward pass for ReLU layer.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Defining function for computing forward pass for ReLU layer.
ReLU layer - this is rectified linear units layer.
Input consists of following:
    x - input data of any shape.
Function returns a tuple of:
    relu_output - output data of the same shape as x,
    cache - is x, needed in backward pass.
"""</span>
</code></pre></div></div>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c"># Cache for output</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c"># Implementing ReLU forward pass</span>
    <span class="c"># Numbers that are less than zero will be changed to 0</span>
    <span class="n">relu_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c"># Returning calculated ReLU output</span>
    <span class="k">return</span> <span class="n">relu_output</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>

<h4 id="backward-pass-for-relu-layer"><a id="backward-pass-for-relu-layer">Backward Pass for ReLU layer</a></h4>
<p>Defining function for backward pass for ReLU layer.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Defining function for computing backward pass for ReLU layer.
ReLU layer - this is rectified linear units layer.
Input consists of following:
    derivatives_out - Upstream derivatives of any shape,
    cache - is x, of the same shape as derivatives_out.
Function returns:
    dx - gradient with respect to x.
"""</span>
</code></pre></div></div>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">relu_backward</span><span class="p">(</span><span class="n">derivatives_out</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="c"># Preparing variable for input from cache</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cache</span>

    <span class="c"># Implementing backward pass for ReLU layer</span>
    <span class="c"># Creating array with the same shape as x</span>
    <span class="c"># Filling with 'True' and 'False' according to the condition 'x &gt; 0'</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="c"># Calculating gradient with respect to x</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">temp</span> <span class="o">*</span> <span class="n">derivatives_out</span>

    <span class="c"># Backward pass for ReLU layer will return gradient with respect to x</span>
    <span class="c"># Each element of the array will be filled with '0'</span>
    <span class="c"># or derivative if that value in x was more than 0</span>

    <span class="c"># Returning calculated ReLU output</span>
    <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h4 id="softmax-classification-loss"><a id="softmax-classification-loss">Softmax Classification loss</a></h4>
<p>Defining function for Softmax Classification loss.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Defining function for computing Logarithmic loss and gradient for Softmax Classification.
Input consists of following:
    x - input data of shape (N, C),
        where x[i, j] is score for the j-th class for the i-th input. 
    y - vector of labels of shape (N,),
        where y[i] is the label for x[i] and 0 &lt;= y[i] &lt; C.
Function returns:
    loss - scalar giving the Logarithmic loss,
    dx - gradient of loss with respect to x.
"""</span>
</code></pre></div></div>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a>)</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c"># Calculating probabilities</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="n">probabilities</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c"># Getting number of samples</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c"># Calculating Logarithmic loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="p">]))</span> <span class="o">/</span> <span class="n">N</span>

    <span class="c"># Calculating gradient</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">probabilities</span>
    <span class="n">dx</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="n">dx</span> <span class="o">/=</span> <span class="n">N</span>

    <span class="c"># Returning tuple of Logarithmic loss and gradient</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dx</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="creating-classifier---model-of-cnn"><a id="creating-classifier-model-of-cnn">Creating Classifier - model of CNN</a></h3>
<p>Creating model of CNN Classifier:</p>
<ul>
  <li>Creating class for ConvNet1</li>
  <li>Initializing new Network</li>
  <li>Evaluating loss for training ConvNet1</li>
  <li>Calculating scores for predicting ConvNet1</li>
</ul>

<h4 id="creating-class-and-initializing-new-network"><a id="initializing-new-network">Creating Class and Initializing new Network</a></h4>
<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Classifiers/ConvNet1.py">ConvNet1.py</a>)</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""""""""
Initializing ConvNet1 with following architecture:
Conv - ReLU - Pooling - Affine - ReLU - Affine - Softmax
Neural Network operates on mini-batches of data of shape (N, C, H, W),
N is number of images, each with C channels, height H and width W.
"""</span>

<span class="c"># Creating class for ConvNet1</span>
<span class="k">class</span> <span class="nc">ConvNet1</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="s">"""""""""
    Initializing new Network
    Input consists of following:
        input_dimension - tuple of shape (C, H, W) giving size of input data,
        number_of_filters - number of filters to use in Convolutional layer (which is only one here),
        size_of_filter - size of filter to use in the Convolutional layer (which is only one here),
        hidden_dimension - number of neurons to use in the Fully-Connected hidden layer,
        number_of_classes - number of scores to produce from the final Affine layer,
        weight_scale - scalar giving standard deviation for random initialization of weights,
        regularization - scala giving L2 regularization strength,
        dtype - numpy datatype to use for computation.
        
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dimension</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">number_of_filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">size_of_filter</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                 <span class="n">hidden_dimension</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">number_of_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">weight_scale</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">regularization</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>

        <span class="c"># Defining dictionary to store all weights and biases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c"># Defining variable for regularization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">=</span> <span class="n">regularization</span>
        <span class="c"># Defining datatype for computation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="c"># Getting input dimension C - channels, H - height, W - width</span>
        <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">input_dimension</span>
        <span class="c"># Getting filter size which is squared</span>
        <span class="n">HH</span> <span class="o">=</span> <span class="n">WW</span> <span class="o">=</span> <span class="n">size_of_filter</span>
        <span class="c"># Getting number of filters</span>
        <span class="n">F</span> <span class="o">=</span> <span class="n">number_of_filters</span>
        <span class="c"># Getting number of neurons in hidden Affine layer</span>
        <span class="n">Hh</span> <span class="o">=</span> <span class="n">hidden_dimension</span>
        <span class="c"># Getting number of classes in output Affine layer</span>
        <span class="n">Hclass</span> <span class="o">=</span> <span class="n">number_of_classes</span>

        <span class="c"># Initializing weights and biases for Convolutional layer (which is only one here)</span>
        <span class="c"># Weights are the volume of shape (F, C, HH, WW).</span>
        <span class="c"># Where F is number of filters, each with C channels, height HH and width WW.</span>
        <span class="c"># Biases initialized with 0 and shape (F,)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'w1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">HH</span><span class="p">,</span> <span class="n">WW</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">F</span><span class="p">)</span>

        <span class="s">"""
        Defining parameters for Convolutional layer (which is only one here):
            'cnn_params' is a dictionary with following keys:
                'stride' - step for sliding,
                'pad' - zero-pad frame around input that is calculated by following formula:
                    pad = (size_of_filter - 1) / 2
        
        Calculating spatial size of output image volume (feature maps) by following formulas:
            feature_maps - output data of feature maps of shape (N, F, Hc, Wc) where:
                Hc = 1 + (H + 2 * pad - HH) / stride
                Wc = 1 + (W + 2 * pad - WW) / stride
                    where,
                    N here is the same as we have it as number of input images,
                    F here is as number of channels of each N (that are now as feature maps),
                    HH and WW are height and width of filter.
        
        Input for CNN layer has shape of (N, C, H, W)
        Output from CNN layer has shape of (N, F, Hc, Wc)
        """</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cnn_params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'stride'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'pad'</span><span class="p">:</span> <span class="nb">int</span><span class="p">((</span><span class="n">size_of_filter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)}</span>
        <span class="n">Hc</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">cnn_params</span><span class="p">[</span><span class="s">'pad'</span><span class="p">]</span> <span class="o">-</span> <span class="n">HH</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">cnn_params</span><span class="p">[</span><span class="s">'stride'</span><span class="p">])</span>
        <span class="n">Wc</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">cnn_params</span><span class="p">[</span><span class="s">'pad'</span><span class="p">]</span> <span class="o">-</span> <span class="n">WW</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">cnn_params</span><span class="p">[</span><span class="s">'stride'</span><span class="p">])</span>

        <span class="s">"""
        Defining parameters for Max Pooling layer:
            'pooling_params' is a dictionary with following keys:
                'pooling_height' - height of pooling region,
                'pooling_width' - width of pooling region,
                'stride' - step (distance) between pooling regions.
    
        Calculating spatial size of output image volume after Max Pooling layer
        by following formulas:
            output resulted data of shape (N, C, Hp, Wp) where:
                Hp = 1 + (Hc - pooling_height) / stride
                Wp = 1 + (Wc - pooling_width) / stride
                    where,
                    N here is the same as we have it as number of filters,
                    C here is as number of channels of each N,
                    Hc and Wc are height and width of output feature maps
                    from Convolutional layer.
                    
        Input for Max Pooling layer has shape of (N, F, Hc, Wc)
        Output from Max Pooling layer has shape of (N, F, Hp, Wp)
        """</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pooling_params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'pooling_height'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">'pooling_width'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">'stride'</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
        <span class="n">Hp</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">Hc</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_params</span><span class="p">[</span><span class="s">'pooling_height'</span><span class="p">])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_params</span><span class="p">[</span><span class="s">'stride'</span><span class="p">])</span>
        <span class="n">Wp</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">Wc</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_params</span><span class="p">[</span><span class="s">'pooling_width'</span><span class="p">])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_params</span><span class="p">[</span><span class="s">'stride'</span><span class="p">])</span>

        <span class="s">"""
        Input for hidden Affine layer has shape of (N, F * Hp * Wp)
        Output from hidden Affine layer has shape of (N, Hh)
        """</span>

        <span class="c"># Initializing weights and biases for Affine hidden layer</span>
        <span class="c"># Weights are the volume of shape (F * Hp * Wp, Hh)</span>
        <span class="c"># Where F * Hp * Wp performs full connections from Max Pooling layer to Affine hidden layer</span>
        <span class="c"># Hh is number of neurons</span>
        <span class="c"># Biases initialized with 0 and shape (Hh,)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'w2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">F</span> <span class="o">*</span> <span class="n">Hp</span> <span class="o">*</span> <span class="n">Wp</span><span class="p">,</span> <span class="n">Hh</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Hh</span><span class="p">)</span>

        <span class="s">"""
        Input for output Affine layer has shape of (N, Hh)
        Output from output Affine layer has shape of (N, Hclass)
        """</span>

        <span class="c"># Initializing weights and biases for output Affine layer</span>
        <span class="c"># Weights are the volume of shape (Hh, Hclass)</span>
        <span class="c"># Weights perform full connections from hidden to output layer</span>
        <span class="c"># Hclass is number of neurons</span>
        <span class="c"># Biases initialized with 0 and shape (Hh,)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'w3'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">Hh</span><span class="p">,</span> <span class="n">Hclass</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b3'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Hclass</span><span class="p">)</span>

        <span class="c"># After initialization of Neural Network is done it is needed to set values as 'dtype'</span>
        <span class="c"># Going through all keys from dictionary</span>
        <span class="c"># Setting to all values needed 'dtype'</span>

        <span class="k">for</span> <span class="n">d_key</span><span class="p">,</span> <span class="n">d_value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">d_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_value</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="evaluating-loss-for-training-convnet1"><a id="evaluating-loss-for-training-convnet1">Evaluating loss for training ConvNet1</a></h4>
<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Classifiers/ConvNet1.py">ConvNet1.py</a>)</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="s">"""
    Evaluating loss for training ConvNet1.
    Input consists of following:
        x of shape (N, C, H, W) - N data, each with C channels, height H and width W.
        y - vector of labels of shape (N,), where y[i] is the label for x[i].
    Function returns:      
        loss - scalar giving the Logarithmic loss,
        gradients - dictionary with the same keys as self.params,
                    mapping parameter names to gradients of loss
                    with respect to those parameters.
        
    """</span>

    <span class="k">def</span> <span class="nf">loss_for_training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c"># Getting weights and biases</span>
        <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'w1'</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span>
        <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'w2'</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span>
        <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'w3'</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b3'</span><span class="p">]</span>

        <span class="c"># Implementing forward pass for ConvNet1 and computing class scores</span>
        <span class="c"># Forward pass for Conv - ReLU - Pooling - Affine - ReLU - Affine</span>
        <span class="n">cnn_output</span><span class="p">,</span> <span class="n">cache_cnn</span> <span class="o">=</span> <span class="n">cnn_forward_naive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cnn_params</span><span class="p">)</span>
        <span class="n">relu_output_1</span><span class="p">,</span> <span class="n">cache_relu_1</span> <span class="o">=</span> <span class="n">relu_forward</span><span class="p">(</span><span class="n">cnn_output</span><span class="p">)</span>
        <span class="n">pooling_output</span><span class="p">,</span> <span class="n">cache_pooling</span> <span class="o">=</span> <span class="n">max_pooling_forward_naive</span><span class="p">(</span><span class="n">relu_output_1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_params</span><span class="p">)</span>
        <span class="n">affine_hidden</span><span class="p">,</span> <span class="n">cache_affine_hidden</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">pooling_output</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
        <span class="n">relu_output_2</span><span class="p">,</span> <span class="n">cache_relu_2</span> <span class="o">=</span> <span class="n">relu_forward</span><span class="p">(</span><span class="n">affine_hidden</span><span class="p">)</span>
        <span class="n">scores</span><span class="p">,</span> <span class="n">cache_affine_output</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">relu_output_2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">)</span>

        <span class="c"># Implementing backward pass for ConvNet1 and computing loss and gradients</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">d_scores</span> <span class="o">=</span> <span class="n">softmax_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c"># Adding L2 regularization</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">w1</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">w2</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">w3</span><span class="p">))</span>

        <span class="c"># Backward pass through Affine output</span>
        <span class="n">dx3</span><span class="p">,</span> <span class="n">dw3</span><span class="p">,</span> <span class="n">db3</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">d_scores</span><span class="p">,</span> <span class="n">cache_affine_output</span><span class="p">)</span>
        <span class="c"># Adding L2 regularization</span>
        <span class="n">dw3</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">w3</span>

        <span class="c"># Backward pass through ReLu and Affine hidden</span>
        <span class="n">d_relu_2</span> <span class="o">=</span> <span class="n">relu_backward</span><span class="p">(</span><span class="n">dx3</span><span class="p">,</span> <span class="n">cache_relu_2</span><span class="p">)</span>
        <span class="n">dx2</span><span class="p">,</span> <span class="n">dw2</span><span class="p">,</span> <span class="n">db2</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">d_relu_2</span><span class="p">,</span> <span class="n">cache_affine_hidden</span><span class="p">)</span>
        <span class="c"># Adding L2 regularization</span>
        <span class="n">dw2</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">w2</span>

        <span class="c"># Backward pass through Pooling, ReLu and Conv</span>
        <span class="n">d_pooling</span> <span class="o">=</span> <span class="n">max_pooling_backward_naive</span><span class="p">(</span><span class="n">dx2</span><span class="p">,</span> <span class="n">cache_pooling</span><span class="p">)</span>
        <span class="n">d_relu_1</span> <span class="o">=</span> <span class="n">relu_backward</span><span class="p">(</span><span class="n">d_pooling</span><span class="p">,</span> <span class="n">cache_relu_1</span><span class="p">)</span>
        <span class="n">dx1</span><span class="p">,</span> <span class="n">dw1</span><span class="p">,</span> <span class="n">db1</span> <span class="o">=</span> <span class="n">cnn_backward_naive</span><span class="p">(</span><span class="n">d_relu_1</span><span class="p">,</span> <span class="n">cache_cnn</span><span class="p">)</span>
        <span class="c"># Adding L2 regularization</span>
        <span class="n">dw1</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">*</span> <span class="n">w1</span>

        <span class="c"># Putting resulted derivatives into gradient dictionary</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">gradients</span><span class="p">[</span><span class="s">'w1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dw1</span>
        <span class="n">gradients</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">db1</span>
        <span class="n">gradients</span><span class="p">[</span><span class="s">'w2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dw2</span>
        <span class="n">gradients</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">db2</span>
        <span class="n">gradients</span><span class="p">[</span><span class="s">'w3'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dw3</span>
        <span class="n">gradients</span><span class="p">[</span><span class="s">'b3'</span><span class="p">]</span> <span class="o">=</span> <span class="n">db3</span>

        <span class="c"># Returning loss and gradients</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">gradients</span>
</code></pre></div></div>

<h4 id="calculating-scores-for-predicting-convnet1"><a id="calculating-scores-for-predicting-convnet1">Calculating scores for predicting ConvNet1</a></h4>
<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Classifiers/ConvNet1.py">ConvNet1.py</a>)</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
    Calculating scores for predicting ConvNet1.
    Input consists of following:
        x of shape (N, C, H, W) - N data, each with C channels, height H and width W.
    Function returns:
        scores - array of shape (N, C) giving classification scores,
                 where scores[i, C] is the classification score for x[i] and class C.
    """</span>
    <span class="k">def</span> <span class="nf">scores_for_predicting</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c"># Getting weights and biases</span>
        <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'w1'</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span>
        <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'w2'</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span>
        <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'w3'</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b3'</span><span class="p">]</span>

        <span class="c"># Implementing forward pass for ConvNet1 and computing class scores</span>
        <span class="c"># Forward pass for Conv - ReLU - Pooling - Affine - ReLU - Affine</span>
        <span class="n">cnn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cnn_forward_naive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cnn_params</span><span class="p">)</span>
        <span class="n">relu_output_1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">relu_forward</span><span class="p">(</span><span class="n">cnn_output</span><span class="p">)</span>
        <span class="n">pooling_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">max_pooling_forward_naive</span><span class="p">(</span><span class="n">relu_output_1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_params</span><span class="p">)</span>
        <span class="n">affine_hidden</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">pooling_output</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
        <span class="n">relu_output_2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">relu_forward</span><span class="p">(</span><span class="n">affine_hidden</span><span class="p">)</span>
        <span class="n">scores</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">relu_output_2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">)</span>

        <span class="c"># Returning scores</span>
        <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="defining-functions-for-optimization"><a id="optimization-functions">Defining Functions for Optimization</a></h3>
<p>Using different types of optimization rules to update parameters of the Model.</p>

<h4 id="vanilla-sgd-updating-method"><a id="vanilla-sgd">Vanilla SGD updating method</a></h4>
<p>Rule for updating parameters is as following:</p>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/vanilla_sgd.png" alt="Vanilla SGD" /></p>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/optimize_rules.py">optimize_rules.py</a>)</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Creating function for parameters updating based on Vanilla SGD</span>
<span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c"># Checking if there was not passed any configuration</span>
    <span class="c"># Then, creating config as dictionary</span>
    <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c"># Assigning to 'learning_rate' value by default</span>
    <span class="c"># If 'learning_rate' was passed in config dictionary, then this will not influence</span>
    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s">'learning_rate'</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
	
    <span class="c"># Implementing update rule as Vanilla SGD</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="n">config</span><span class="p">[</span><span class="s">'learning_rate'</span><span class="p">]</span> <span class="o">*</span> <span class="n">dw</span>
	
    <span class="c"># Returning updated parameter and configuration</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">config</span>
</code></pre></div></div>

<h4 id="momentum-sgd-updating-method"><a id="momentum-sgd">Momentum SGD updating method</a></h4>
<p>Rule for updating parameters is as following:</p>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/momentum_sgd.png" alt="Momentum SGD" /></p>

<h4 id="rms-propagation-updating-method"><a id="rms-propagation">RMS Propagation updating method</a></h4>
<p>Rule for updating parameters is as following:</p>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/rms.png" alt="RMS" /></p>

<p><br /></p>

<h3 id="creating-solver-class"><a id="creating-solver-class">Creating Solver Class</a></h3>
<p>Creating Solver class for training classification models and for predicting:</p>
<ul>
  <li>Creating and Initializing class for Solver</li>
  <li>Creating ‘reset’ function for defining variables for optimization</li>
  <li>Creating function ‘step’ for making single gradient update</li>
  <li>Creating function for checking accuracy of the model on the current provided data</li>
  <li>Creating function for training the model</li>
</ul>

<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Solver.py">Solver.py</a>)</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Creating class for Solver</span>
<span class="k">class</span> <span class="nc">Solver</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="s">"""""""""
    Initializing new Solver instance
    Input consists of following required and Optional arguments.
    
    Required arguments consist of following:
        model - a modal object conforming parameters as described above,
        data - a dictionary with training and validating data.
    
    Optional arguments (**kwargs) consist of following:
        update_rule - a string giving the name of an update rule in optimize_rules.py,
        optimization_config - a dictionary containing hyperparameters that will be passed 
                              to the chosen update rule. Each update rule requires different
                              parameters, but all update rules require a 'learning_rate' parameter.
        learning_rate_decay - a scalar for learning rate decay. After each epoch the 'learning_rate'
                              is multiplied by this value,
        batch_size - size of minibatches used to compute loss and gradients during training,
        number_of_epochs - the number of epoch to run for during training,
        print_every - integer number that corresponds to printing loss every 'print_every' iterations,
        verbose_mode - boolean that corresponds to condition whether to print details or not. 

    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c"># Preparing required arguments</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'x_train'</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'y_train'</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_validation</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'x_validation'</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_validation</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'y_validation'</span><span class="p">]</span>

        <span class="c"># Preparing optional arguments</span>
        <span class="c"># Unpacking keywords of arguments</span>
        <span class="c"># Using 'pop' method and setting at the same time default value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_rule</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s">'update_rule'</span><span class="p">,</span> <span class="s">'sgd'</span><span class="p">)</span>  <span class="c"># Default is 'sgd'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimization_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s">'optimization_config'</span><span class="p">,</span> <span class="p">{})</span>  <span class="c"># Default is '{}'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate_decay</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s">'learning_rate_decay'</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>  <span class="c"># Default is '1.0'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s">'batch_size'</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c"># Default is '100'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">number_of_epochs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s">'number_of_epochs'</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c"># Default is '10'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print_every</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s">'print_every'</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c"># Default is '10'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mode</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s">'verbose_mode'</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>  <span class="c"># Default is 'True'</span>

        <span class="c"># Checking if there are extra keyword arguments and raising an error</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">extra</span> <span class="o">=</span> <span class="s">', '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'Extra argument:'</span><span class="p">,</span> <span class="n">extra</span><span class="p">)</span>

        <span class="c"># Checking if update rule exists and raising an error if not</span>
        <span class="c"># Using function 'hasattr(object, name)',</span>
        <span class="c"># where 'object' is our imported module 'optimize_rules'</span>
        <span class="c"># and 'name' is the name of the function inside</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">optimize_rules</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_rule</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'Update rule'</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_rule</span><span class="p">,</span> <span class="s">'does not exists'</span><span class="p">)</span>

        <span class="c"># Reassigning string 'self.update_rule' with the real function from 'optimize_rules'</span>
        <span class="c"># Using function 'getattr(object, name)',</span>
        <span class="c"># where 'object' is our imported module 'optimize_rules'</span>
        <span class="c"># and 'name' is the name of the function inside</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_rule</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">optimize_rules</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_rule</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="defining-function-with-additional-variables"><a id="reset">Defining function with additional variables</a></h4>
<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Solver.py">Solver.py</a>)</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c"># Creating 'reset' function for defining variables for optimization</span>
    <span class="k">def</span> <span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c"># Setting up variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_validation_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_accuracy_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_accuracy_history</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c"># Making deep copy of 'optimization_config' for every parameter at every layer</span>
        <span class="c"># It means that at least learning rate will be for every parameter at every layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimization_configurations</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_config</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimization_configurations</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span>
</code></pre></div></div>

<h4 id="defining-function-for-making-single-step"><a id="step">Defining function for making single step</a></h4>
<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Solver.py">Solver.py</a>)</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c"># Creating function 'step' for making single gradient update</span>
    <span class="k">def</span> <span class="nf">_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c"># Making minibatch from training data</span>
        <span class="c"># Getting total number of training images</span>
        <span class="n">number_of_training_images</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c"># Getting random batch of 'batch_size' size from total number of training images</span>
        <span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">number_of_training_images</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="c"># Getting training dataset according to the 'batch_mask'</span>
        <span class="n">x_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>

        <span class="c"># Calculating loss and gradient for current minibatch</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_for_training</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>

        <span class="c"># Adding calculated loss to the history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="c"># Implementing updating for all parameters (weights and biases)</span>
        <span class="c"># Going through all parameters</span>
        <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c"># Taking current value of derivative for current parameter</span>
            <span class="n">dw</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
            <span class="c"># Defining configuration for current parameter</span>
            <span class="n">config_for_current_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_configurations</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
            <span class="c"># Implementing updating and getting next values</span>
            <span class="n">next_w</span><span class="p">,</span> <span class="n">next_configuration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_rule</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">config_for_current_p</span><span class="p">)</span>
            <span class="c"># Updating value in 'params'</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_w</span>
            <span class="c"># Updating value in 'optimization_configurations'</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimization_configurations</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_configuration</span>
</code></pre></div></div>

<h4 id="defining-function-for-checking-accuracy"><a id="accuracy">Defining function for checking accuracy</a></h4>
<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Solver.py">Solver.py</a>)</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c"># Creating function for checking accuracy of the model on the current provided data</span>
    <span class="c"># Accuracy will be used in 'train' function for both training dataset and for testing dataset</span>
    <span class="c"># Depending on which input into the model will be provided</span>
    <span class="k">def</span> <span class="nf">check_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">number_of_samples</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>

        <span class="s">"""""""""
        Input consists of following:
            x of shape (N, C, H, W) - N data, each with C channels, height H and width W,
            y - vector of labels of shape (N,),
            number_of_samples - subsample data and test model only on this number of data,
            batch_size - split x and y into batches of this size to avoid using too much memory.

        Function returns:
            accuracy - scalar number giving percentage of images 
                       that were correctly classified by model.
        """</span>

        <span class="c"># Getting number of input images</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c"># Subsample data if 'number_of_samples' is not None</span>
        <span class="c"># and number of input images is more than 'number_of_samples'</span>
        <span class="k">if</span> <span class="n">number_of_samples</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">N</span> <span class="o">&gt;</span> <span class="n">number_of_samples</span><span class="p">:</span>
            <span class="c"># Getting random batch of 'number_of_samples' size from total number of input images</span>
            <span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">number_of_samples</span><span class="p">)</span>
            <span class="c"># Reassigning (decreasing) N to 'number_of_samples'</span>
            <span class="n">N</span> <span class="o">=</span> <span class="n">number_of_samples</span>
            <span class="c"># Getting dataset for checking accuracy according to the 'batch_mask'</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>

        <span class="c"># Defining and calculating number of batches</span>
        <span class="c"># Also, making it as integer with 'int()'</span>
        <span class="n">number_of_batches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="c"># Increasing number of batches if there is no exact match of input images over 'batch_size'</span>
        <span class="k">if</span> <span class="n">N</span> <span class="o">%</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">number_of_batches</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c"># Defining variable for storing predicted class for appropriate input image</span>
        <span class="n">y_predicted</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c"># Computing predictions in batches</span>
        <span class="c"># Going through all batches defined by 'number_of_batches'</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_batches</span><span class="p">):</span>
            <span class="c"># Defining start index and end index for current batch of images</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span>
            <span class="n">e</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
            <span class="c"># Getting scores by calling function 'loss_for predicting' from model</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">scores_for_predicting</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">s</span><span class="p">:</span><span class="n">e</span><span class="p">])</span>
            <span class="c"># Appending result to the list 'y_predicted'</span>
            <span class="c"># Scores is given for each image with 10 numbers of predictions for each class</span>
            <span class="c"># Getting only one class for each image with maximum value</span>
            <span class="n">y_predicted</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
            <span class="c"># Example</span>
            <span class="c">#</span>
            <span class="c"># a = np.arange(6).reshape(2, 3)</span>
            <span class="c"># print(a)</span>
            <span class="c">#    ([[0, 1, 2],</span>
            <span class="c">#     [3, 4, 5]])</span>
            <span class="c">#</span>
            <span class="c"># print(np.argmax(a))</span>
            <span class="c"># 5</span>
            <span class="c">#</span>
            <span class="c"># np.argmax(a, axis=0)</span>
            <span class="c">#     ([1, 1, 1])</span>
            <span class="c">#</span>
            <span class="c"># np.argmax(a, axis=1)</span>
            <span class="c">#     ([2, 2])</span>
            <span class="c">#</span>
            <span class="c"># Now we have each image with its only one predicted class (index of each row)</span>
            <span class="c"># but not with 10 numbers for each class</span>

        <span class="c"># Concatenating list of lists and making it as numpy array</span>
        <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">)</span>

        <span class="c"># Finally, we compare predicted class with correct class for all input images</span>
        <span class="c"># And calculating mean value among all values of following numpy array</span>
        <span class="c"># By saying 'y_predicted == y' we create numpy array with True and False values</span>
        <span class="c"># 'np.mean' function will return average of the array elements</span>
        <span class="c"># The average is taken over the flattened array by default</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_predicted</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span>

        <span class="c"># Returning accuracy</span>
        <span class="k">return</span> <span class="n">accuracy</span>
</code></pre></div></div>

<h4 id="defining-function-for-running-training-procedure"><a id="train">Defining function for running training procedure</a></h4>
<p>Consider following part of the code:
<br />(related file: <a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Solver.py">Solver.py</a>)</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c"># Creating function for training the model</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c"># Getting total number of training images</span>
        <span class="n">number_of_training_images</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c"># Calculating number of iterations per one epoch</span>
        <span class="c"># If 'number_of_training_images' is less than 'self.batch_size' then we chose '1'</span>
        <span class="n">iterations_per_one_epoch</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">number_of_training_images</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c"># Calculating total number of iterations for all process of training</span>
        <span class="c"># Also, making it as integer with 'int()'</span>
        <span class="n">iterations_total</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_epochs</span> <span class="o">*</span> <span class="n">iterations_per_one_epoch</span><span class="p">)</span>

        <span class="c"># Running training process in the loop for total number of iterations</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations_total</span><span class="p">):</span>
            <span class="c"># Making single step for updating all parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_step</span><span class="p">()</span>

            <span class="c"># Checking if training loss has to be print every 'print_every' iteration</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mode</span> <span class="ow">and</span> <span class="n">t</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c"># Printing current iteration and showing total number of iterations</span>
                <span class="c"># Printing currently saved loss from loss history</span>
                <span class="k">print</span><span class="p">(</span><span class="s">'Iteration: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s">'/'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">iterations_total</span><span class="p">)</span> <span class="o">+</span> <span class="s">','</span><span class="p">,</span>
                      <span class="s">'loss ='</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

            <span class="c"># Defining variable for checking end of current epoch</span>
            <span class="n">end_of_current_epoch</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">iterations_per_one_epoch</span> <span class="o">==</span> <span class="mi">0</span>

            <span class="c"># Checking if it is the end of current epoch</span>
            <span class="k">if</span> <span class="n">end_of_current_epoch</span><span class="p">:</span>
                <span class="c"># Incrementing epoch counter</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="c"># Decaying learning rate for every parameter at every layer</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_configurations</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimization_configurations</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="s">'learning_rate'</span><span class="p">]</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate_decay</span>

            <span class="c"># Defining variables for first and last iterations</span>
            <span class="n">first_iteration</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">last_iteration</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">==</span> <span class="n">iterations_total</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c"># Checking training and validation accuracy</span>
            <span class="c"># At the first iteration, the last iteration, and at the end of every epoch</span>
            <span class="k">if</span> <span class="n">first_iteration</span> <span class="ow">or</span> <span class="n">last_iteration</span> <span class="ow">or</span> <span class="n">end_of_current_epoch</span><span class="p">:</span>
                <span class="c"># Checking training accuracy with 1000 samples</span>
                <span class="n">training_accuracy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span>
                                                        <span class="n">number_of_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

                <span class="c"># Checking validation accuracy</span>
                <span class="c"># We don't specify number of samples as it has only 1000 images itself</span>
                <span class="n">validation_accuracy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_validation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_validation</span><span class="p">)</span>

                <span class="c"># Adding calculated accuracy to the history</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_accuracy_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">validation_accuracy_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">validation_accuracy</span><span class="p">)</span>

                <span class="c"># Checking if the 'verbose_mode' is 'True' then printing details</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mode</span><span class="p">:</span>
                    <span class="c"># Printing current epoch over total amount of epochs</span>
                    <span class="c"># And training and validation accuracy</span>
                    <span class="k">print</span><span class="p">(</span><span class="s">'Epoch: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">)</span> <span class="o">+</span> <span class="s">'/'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_epochs</span><span class="p">)</span> <span class="o">+</span> <span class="s">','</span><span class="p">,</span>
                          <span class="s">'Training accuracy = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">)</span> <span class="o">+</span> <span class="s">','</span><span class="p">,</span>
                          <span class="s">'Validation accuracy = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">validation_accuracy</span><span class="p">))</span>

                <span class="c"># Tracking the best model parameters by comparing validation accuracy</span>
                <span class="k">if</span> <span class="n">validation_accuracy</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_validation_accuracy</span><span class="p">:</span>
                    <span class="c"># Assigning current validation accuracy to the best validation accuracy</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">best_validation_accuracy</span> <span class="o">=</span> <span class="n">validation_accuracy</span>
                    <span class="c"># Reset 'self.best_params' dictionary</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">best_params</span> <span class="o">=</span> <span class="p">{}</span>
                    <span class="c"># Assigning current parameters to the best parameters variable</span>
                    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">best_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="c"># At the end of training process swapping best parameters to the model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_params</span>

        <span class="c"># Saving trained model parameters into 'pickle' file</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'Serialized_Models/model_params_ConvNet1.pickle'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

        <span class="c"># Saving loss, training accuracy and validation accuracy histories into 'pickle' file</span>
        <span class="n">history_dictionary</span> <span class="o">=</span> <span class="p">{</span><span class="s">'loss_history'</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span><span class="p">,</span>
                              <span class="s">'train_accuracy_history'</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_accuracy_history</span><span class="p">,</span>
                              <span class="s">'validation_history'</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_accuracy_history</span><span class="p">}</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'Serialized_Models/model_histories_ConvNet1.pickle'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">history_dictionary</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="overfitting-small-data"><a id="overfitting-small-data">Overfitting Small Data</a></h3>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c"># Importing module 'ConvNet1.py'</span>
<span class="kn">from</span> <span class="nn">Helper_Functions.ConvNet1</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c"># Importing module 'Solver.py'</span>
<span class="kn">from</span> <span class="nn">Solver</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c"># Loading data</span>
<span class="c"># Opening file for reading in binary mode</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'Data_Preprocessing/data.pickle'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'latin1'</span><span class="p">)</span>  <span class="c"># dictionary type</span>

<span class="c"># Number of training examples</span>
<span class="n">number_of_training_data</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c"># Can be changed and study with just 10 examples</span>

<span class="c"># Preparing data by slicing in 'data' dictionary appropriate array</span>
<span class="n">small_data</span> <span class="o">=</span> <span class="p">{</span>
             <span class="s">'x_train'</span><span class="p">:</span><span class="n">d</span><span class="p">[</span><span class="s">'x_train'</span><span class="p">][:</span><span class="n">number_of_training_data</span><span class="p">],</span>
             <span class="s">'y_train'</span><span class="p">:</span><span class="n">d</span><span class="p">[</span><span class="s">'y_train'</span><span class="p">][:</span><span class="n">number_of_training_data</span><span class="p">],</span>
             <span class="s">'x_validation'</span><span class="p">:</span><span class="n">d</span><span class="p">[</span><span class="s">'x_validation'</span><span class="p">],</span>
             <span class="s">'y_validation'</span><span class="p">:</span><span class="n">d</span><span class="p">[</span><span class="s">'y_validation'</span><span class="p">]</span>
             <span class="p">}</span>

<span class="c"># Creating instance of class for 'ConvNet1' and initializing model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ConvNet1</span><span class="p">(</span><span class="n">input_dimension</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">weight_scale</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">hidden_dimension</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c"># Creating instance of class for 'Solver' and initializing model</span>
<span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                <span class="n">small_data</span><span class="p">,</span>
                <span class="n">update_rule</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span>
                <span class="n">optimization_config</span><span class="o">=</span><span class="p">{</span><span class="s">'learning_rate'</span><span class="p">:</span><span class="mf">1e-3</span><span class="p">},</span>
                <span class="n">learning_rate_decay</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                <span class="n">number_of_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>  <span class="c"># Can be changed and study with just 40 epochs</span>
                <span class="n">print_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">verbose_mode</span><span class="o">=</span><span class="bp">True</span>
               <span class="p">)</span>

<span class="c"># Running training process</span>
<span class="n">solver</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/overfitting_small_data_model_1_mnist.png" alt="Overfitting Small Data" /></p>

<p><br /></p>

<h3 id="training-results"><a id="training-results">Training Results</a></h3>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c"># Importing module 'ConvNet1.py'</span>
<span class="kn">from</span> <span class="nn">Helper_Functions.ConvNet1</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c"># Importing module 'Solver.py'</span>
<span class="kn">from</span> <span class="nn">Solver</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c"># Loading data</span>
<span class="c"># Opening file for reading in binary mode</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'Data_Preprocessing/data.pickle'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'latin1'</span><span class="p">)</span>  <span class="c"># dictionary type</span>

<span class="c"># Creating instance of class for 'ConvNet1' and initializing model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ConvNet1</span><span class="p">(</span><span class="n">weight_scale</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">hidden_dimension</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">regularization</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">e3</span><span class="p">)</span>

<span class="c"># Creating instance of class for 'Solver' and initializing model</span>
<span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                <span class="n">d</span><span class="p">,</span>
                <span class="n">update_rule</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span>
                <span class="n">optimization_config</span><span class="o">=</span><span class="p">{</span><span class="s">'learning_rate'</span><span class="p">:</span><span class="mf">1e-3</span><span class="p">},</span>
                <span class="n">learning_rate_decay</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                <span class="n">number_of_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">print_every</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                <span class="n">verbose_mode</span><span class="o">=</span><span class="bp">True</span>
               <span class="p">)</span>

<span class="c"># Running training process</span>
<span class="n">solver</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div></div>

<p>Training process of Model #1 with 12 000 iterations is shown on the figure below:</p>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/training_model_1_mnist.png" alt="Training Model 1" /></p>

<p>Initialized Filters and Trained Filters for ConvNet Layer is shown on the figure below:</p>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/filters_mnist.png" alt="Filters Cifar10" /></p>

<p>Training process for Filters of ConvNet Layer is shown on the figure below:</p>

<p><img src="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/images/mnist_filters_training.gif" alt="Training Filters Cifar10" /></p>

<p><br /></p>

<h3 id="full-codes-are-available-here"><a id="full-codes">Full codes are available here:</a></h3>
<ul>
  <li>MNIST Digits Classification with <code class="highlighter-rouge">numpy</code> only:
    <ul>
      <li><code class="highlighter-rouge">Data_Preprocessing</code>
        <ul>
          <li><code class="highlighter-rouge">datasets</code></li>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/datasets_preparing.py">datasets_preparing.py</a></li>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Data_Preprocessing/mean_and_std.pickle">mean_and_std.pickle</a></li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">Helper_Functions</code>
        <ul>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/layers.py">layers.py</a></li>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Helper_Functions/optimize_rules.py">optimize_rules.py</a></li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">Classifiers</code>
        <ul>
          <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Classifiers/ConvNet1.py">ConvNet1.py</a></li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">Serialized_Models</code>
        <ul>
          <li>model1.pickle</li>
        </ul>
      </li>
      <li><a href="https://github.com/sichkar-valentyn/Neural_Networks_for_Computer_Vision/blob/master/Codes/Digits_Classification/Solver.py">Solver.py</a></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="mit-license">MIT License</h3>
<h3 id="copyright-c-2018-valentyn-n-sichkar">Copyright (c) 2018 Valentyn N Sichkar</h3>
<h3 id="githubcomsichkar-valentyn">github.com/sichkar-valentyn</h3>
<h3 id="reference-to">Reference to:</h3>
<p>Valentyn N Sichkar. Neural Networks for computer vision in autonomous vehicles and robotics // GitHub platform. DOI: 10.5281/zenodo.1317904</p>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">mnist maintained by <a href="https://github.com/sichkar-valentyn">sichkar-valentyn</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    
  </body>
</html>
